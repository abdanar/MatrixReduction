{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f243fb99-810c-46f2-8d1c-419c8cecf606",
   "metadata": {},
   "source": [
    "The transfer function for the causal LTI system with a realization $(A, B, C, D)$ is given by\n",
    "\\begin{equation*}\n",
    "    H(s) = C(sI_{n} - A)^{-1}B.\n",
    "\\end{equation*}\n",
    "Notice that $H(s) = Cv(s)$, where $v(s)$ is the solution of a parametrized linear coercive model\n",
    "\\begin{equation}\n",
    "    a(v, w; s) = l(w),\n",
    "\\end{equation}\n",
    "where $a(v, w; s) = w^{*}(sI_{n} - A)v$ and $l(w) = w^{*}B$. In other words, we replace the matrix computation of $(sI_{n} - A)^{-1}B$ with a parametrized model (1). However, if one wants to apply this to the Iterative rational Krylov algorithm (IRKA), then the following matrix computations must be done to construct the projection matrices $V$ and $W$:\n",
    "\\begin{equation*}\n",
    "    (-\\mu_{i}I_{n} - A)^{-1}B\\hat{b}_{i}, \\quad \\text{and} \\quad (-\\mu_{i}I_{n} - A)^{-*}C^{T}\\hat{c}_{i} \\quad \\text{for } i = 1,\\ldots,r,\n",
    "\\end{equation*}\n",
    "where $-\\mu_{i}, \\hat{c}_{i}, \\hat{b}_{i}$ are some initial interpolation data and $0 < r \\leq n$ is the desired order of approximating ROM. So, we have decided to solve two parametrized linear coercive models to construct projection matrices $V$ and $W$:\n",
    "\\begin{equation}\n",
    "    a_{1}(v_{1}, w; s) = l_{1}(w) \\quad \\text{and} \\quad a_{2}(v_{2}, w; s) = l_{2}(w),\n",
    "\\end{equation}\n",
    "where $a_{1}(v_{1}, w; s) = w^{*}(sI_{n} - A)v_{1}$ and $l_{1}(w) = w^{*}B$, and $a_{2}(v_{2}, w; s) = w^{*}(sI_{n} - A)^{*}v_{2}$ and $l_{2}(w) = w^{*}C^{T}$, and solutions to these parametrized linear coercive models are\n",
    "\\begin{equation*}\n",
    "    v_{1}(s) = (sI_{n} - A)^{-1}B \\quad \\text{and} \\quad v_{2}(s) = (sI_{n} - A)^{-*}C^{T}.\n",
    "\\end{equation*}\n",
    "Therefore, knowing $v_{1}(\\mu_{i})$ and $v_{2}(\\mu_{i})$ for $i = 1, \\ldots, r$ will suffice for constructing the projection matrices $V$ and $W$. Also, note that these two FOMs are parameter-separable, i.e.,\n",
    "\\begin{equation*}\n",
    "    a_{1}(v_{1},w;s) = w^{*}(sI_{n} - A)v_{1} = sw^{*}I_{n}v_{1} - w^{*}Av_{1}  \\quad  a_{2}(v_{2},w;s) = w^{*}(sI_{n} - A)^{*}v_{2} = \\overline{s}w^{*}I_{n}v_{2} - w^{*}A^{*}v_{2}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "b2d1bcb5-cc0e-4fea-857a-a99f7cb1031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from pymor.basic import *\n",
    "from pymor.models.basic import StationaryModel\n",
    "from pymor.operators.constructions import LincombOperator\n",
    "from pymor.operators.numpy import NumpyMatrixOperator\n",
    "from pymor.parameters.functionals import ProjectionParameterFunctional, ConjugateParameterFunctional\n",
    "from pymor.vectorarrays.numpy import NumpyVectorSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592df60-f621-4eb5-a723-c180b2ba1879",
   "metadata": {},
   "source": [
    "### Constructing a stationary model\n",
    "Let us construct a stationary model of the following parametrized linear coercive model using `pyMOR`:\n",
    "\\begin{equation}\n",
    "    a(v, w; s) = l(w),\n",
    "\\end{equation}\n",
    "where $a(v, w; s) = w^{*}(sI_{n} - A)v$ and $l(w) = w^{*}B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "6e5a5d02-f4fb-4418-9e71-25a089a3ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrixModel(A, B):\n",
    "\n",
    "    '''\n",
    "    This function create a stationary model of the following linear coercive model derived for the given two matrices A and B:\n",
    "    \n",
    "        a(v, w; s) =w^{*}(sI_{n} - A)v and l(w) = w^{*}B.\n",
    "    '''\n",
    "\n",
    "    # Define NumpyMatrixOperators\n",
    "    if isinstance(A, np.ndarray) and isinstance(B, np.ndarray):\n",
    "        dim = A.shape[0]\n",
    "        A_op = NumpyMatrixOperator(A)\n",
    "        B_op = NumpyMatrixOperator(B) # B.shape = (-,1) \n",
    "    else:\n",
    "        dim = to_matrix(A).shape[0]\n",
    "        A_op = A\n",
    "        B_op = B \n",
    "\n",
    "    I_op = NumpyMatrixOperator(np.eye(dim))\n",
    "\n",
    "    # Define parameter functional for 's'\n",
    "    s_param = ProjectionParameterFunctional('s', 1)\n",
    "\n",
    "    # Define bilinear form a(v, w; s) = w^* (sI - A)v\n",
    "    a_op = LincombOperator([I_op, A_op], [s_param, -1])\n",
    "\n",
    "    # Define linear functional l(w) = w^*B\n",
    "    l_op = B_op\n",
    "\n",
    "    # Define the StationaryModel\n",
    "    model = StationaryModel(operator = a_op, rhs = l_op)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2815a64-3885-48f9-ba8c-ac4d582a96b2",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Notice that to make this process more convenient, we provide two options for input matrices. If one has the matrices as `NumPy` arrays, they can import them directly without changing their type to work with `pyMOR`. Additionally, if one wants to input matrices from existing `pyMOR` models, this can also be done directly. To demonstrate this, we provide two examples: one with random matrices that we construct ourselves, and the other with matrices imported from the `penzl_example` in `pymor.models.examples`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "9a187f88-ab03-4c8b-af30-2cb5e9400190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StationaryModel(\n",
       "    LincombOperator(\n",
       "        (NumpyMatrixOperator(<20x20 dense>), NumpyMatrixOperator(<20x20 dense>)),\n",
       "        (ProjectionParameterFunctional('s', index=0), -1)),\n",
       "    NumpyMatrixOperator(<20x1 dense>),\n",
       "    output_functional=ZeroOperator(NumpyVectorSpace(0), NumpyVectorSpace(20)),\n",
       "    products={},\n",
       "    output_d_mu_use_adjoint=True)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stationary model constructed using random numpy arrays\n",
    "\n",
    "# Randomly generated arrays\n",
    "np.random.seed(127)\n",
    "matrixA = np.random.rand(20, 20) # A is 20x20 matrix\n",
    "matrixB = np.random.rand(20).reshape(20,1) # B is 20x1 vector\n",
    "\n",
    "model_numpy = MatrixModel(A = matrixA, B = matrixB)\n",
    "model_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "b936a0a6-c7ce-481f-acb3-2ab87e82799a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c8e5835c474ee3a34a5c35de85c7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(HTML(value='', layout=Layout(height='16em', width='100%')),), titles=('Log Output',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution to the linear coercive model is \n",
      " [[ 1.10896524  1.96032033  0.02339715  0.52739001 -0.35238441 -1.85189312\n",
      "  -0.11570617 -0.4575037   0.55493919 -0.96350068  1.01490125  0.05632634\n",
      "   0.23435721 -0.72291187  0.47195613 -1.1582535  -0.48791287  0.20426283\n",
      "  -0.56944887 -0.30429143]].\n",
      "The exact value of the matrix computation (1.4I - A)^{-1}B is \n",
      " [[ 1.10896524  1.96032033  0.02339715  0.52739001 -0.35238441 -1.85189312\n",
      "  -0.11570617 -0.4575037   0.55493919 -0.96350068  1.01490125  0.05632634\n",
      "   0.23435721 -0.72291187  0.47195613 -1.1582535  -0.48791287  0.20426283\n",
      "  -0.56944887 -0.30429143]].\n",
      "The L-infinity error is 2.4424906541753444e-15.\n"
     ]
    }
   ],
   "source": [
    "# Set parameter for evaluation\n",
    "parameter = {'s': 1.4}  # s = 1.4\n",
    "\n",
    "# Solve the model\n",
    "solution = model_numpy.solve(parameter).to_numpy()\n",
    "\n",
    "# Exact matrix computation\n",
    "exact = (inv(parameter['s']*np.eye(20) - matrixA)@matrixB).reshape(1, 20)\n",
    "\n",
    "# A comparison between the model's result and the exact matrix computation\n",
    "print(f'The solution to the linear coercive model is \\n {solution}.')\n",
    "print(f'The exact value of the matrix computation ({parameter[\"s\"]}I - A)^{{-1}}B is \\n {exact}.')\n",
    "print(f'The L-infinity error is {np.max(abs(exact - solution))}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "6fedee36-5de9-41a0-b802-5df69b135ab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StationaryModel(\n",
       "    LincombOperator(\n",
       "        (NumpyMatrixOperator(<1006x1006 dense>), NumpyMatrixOperator(<1006x1006 sparse, 1012 nnz>)),\n",
       "        (ProjectionParameterFunctional('s', index=0), -1)),\n",
       "    NumpyMatrixOperator(<1006x1 dense>),\n",
       "    output_functional=ZeroOperator(NumpyVectorSpace(0), NumpyVectorSpace(1006)),\n",
       "    products={},\n",
       "    output_d_mu_use_adjoint=True)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stationary model constructed using matrices obtained from penzl example\n",
    "from pymor.models.examples import penzl_example\n",
    "\n",
    "penzl = penzl_example()\n",
    "\n",
    "model_penzl = MatrixModel(penzl.A, penzl.B)\n",
    "model_penzl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "e89aa89f-bc33-474a-8676-0c8f844e48a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9648ceecf9d846d1a40c9ce67cb5da4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(HTML(value='', layout=Layout(height='16em', width='100%')),), titles=('Log Output',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution to the linear coercive model is \n",
      " [[ 0.10004002+2.00080032e-03j -0.10004002+2.00080032e-03j\n",
      "   0.050005  +5.00050005e-04j ...  0.001003  -2.01204612e-06j\n",
      "   0.001002  -2.00801600e-06j  0.001001  -2.00399798e-06j]].\n",
      "The exact value of the matrix computation ((-1+2j)I - A)^{-1}B is \n",
      " [[ 0.10004002+2.00080032e-03j -0.10004002+2.00080032e-03j\n",
      "   0.050005  +5.00050005e-04j ...  0.001003  -2.01204612e-06j\n",
      "   0.001002  -2.00801600e-06j  0.001001  -2.00399798e-06j]].\n",
      "The L-infinity error is 2.7755575615628914e-17.\n"
     ]
    }
   ],
   "source": [
    "from pymor.algorithms.to_matrix import to_matrix\n",
    "\n",
    "# Set parameter for evaluation\n",
    "parameter = {'s': -1 + 2*1j}  # s = - 1 + 2i\n",
    "\n",
    "# Solve the model\n",
    "solution = model_penzl.solve(parameter).to_numpy()\n",
    "\n",
    "# Exact matrix computation\n",
    "matrixA = to_matrix(penzl.A).toarray()\n",
    "matrixB = to_matrix(penzl.B)\n",
    "\n",
    "exact = (inv(parameter['s']*np.eye(penzl.order) - matrixA)@matrixB).reshape(1, penzl.order)\n",
    "\n",
    "# A comparison between the model's result and the exact matrix computation\n",
    "print(f'The solution to the linear coercive model is \\n {solution}.')\n",
    "print(f'The exact value of the matrix computation ({parameter[\"s\"]}I - A)^{{-1}}B is \\n {exact}.')\n",
    "print(f'The L-infinity error is {np.max(abs(exact - solution))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4524c1-8f04-477e-ae98-8d0833a2d987",
   "metadata": {},
   "source": [
    "## Reduced Basis Methods\n",
    "\n",
    "As an example, we will use `penzl_example` from `pymor.models.examples` to construct a reduced basis using the reduced basis methods available in `pyMOR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "7d507872-5ef7-45d3-bd79-b41e3d0cff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.models.examples import penzl_example\n",
    "\n",
    "penzl = penzl_example()\n",
    "model_penzl = MatrixModel(penzl.A, penzl.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bffee-4de6-4e31-8a3f-52807cd936a1",
   "metadata": {},
   "source": [
    "### POD-Galerkin Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "d1790e6b-5614-4256-b6dc-af50775d4d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486d4eb9ecd94bcb960d4b605477cb43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(HTML(value='', layout=Layout(height='16em', width='100%')),), titles=('Log Output',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a parameter space\n",
    "parameter_space = model_penzl.parameters.space(0.01, 10.)\n",
    "\n",
    "# Define a training set\n",
    "training_set = parameter_space.sample_randomly(50)\n",
    "\n",
    "# Compute FOM solutions for the parameters in the training set\n",
    "solution_snapshots = model_penzl.solution_space.empty()\n",
    "for s in training_set:\n",
    "    solution_snapshots.append(model_penzl.solve(s))\n",
    "\n",
    "# Snapshot matrix S\n",
    "snapshot_matrix = solution_snapshots.to_numpy().T\n",
    "print(f'An {snapshot_matrix.shape[0]} by {snapshot_matrix.shape[1]} snapshot matrix is \\n {snapshot_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7156f42-ddfa-408e-844d-d1d8d4c85c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the Singular Value Decomposition (SVD) of the snapshot matrix -> S = UÎ£V^T\n",
    "U, D, Vt = np.linalg.svd(snapshot_matrix, full_matrices = True) # using 'pod_basis, pod_singular_values = pod(solution_snapshots)' gives wrong result (pod_basis._len != 10 (5 != 10)); that's why we don't use it here. \n",
    "\n",
    "# Activate below to automatically select the number of modes based on the energy criterion\n",
    "## The number of modes m should be chosen based on the energy criterion, which ensures that a sufficient portion of the system's total energy (or variance) is captured by the first m modes.\n",
    "#cumulative_energy = np.cumsum(D**2) / np.sum(D**2)\n",
    "\n",
    "##Select the number of modes m to capture at least 95% of the energy\n",
    "#threshold = 0.95  # 95% of the total energy\n",
    "#m = np.argmax(cumulative_energy >= threshold) + 1\n",
    "\n",
    "m = 20 # this is also a reduction order\n",
    "\n",
    "if m > min(snapshot_matrix.shape):\n",
    "    raise ValueError(\"m cannot exceed the rank of the snapshot matrix.\")\n",
    "\n",
    "# The reduced basis (POD basis)\n",
    "pod_basis_numpy = U[:,:m]\n",
    "print(f'The reduced basis (containing the first {m} left singular vectors (POD modes) of the snapshot matrix as its columns) is \\n {pod_basis_numpy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d4cd6-a81c-4825-86df-ba27c8ae6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.reductors.basic import StationaryRBReductor\n",
    "\n",
    "# Convert NumPy array into VectorArray \n",
    "'''\n",
    "The two lines below are the correct way to convert a NumPy reduced basis array into a VectorArray. However, the 'StationaryRBReductor' raises an error because \n",
    "'pod_basis in model_penzl.solution_space' is set to False. This happens because 'pod_basis.dim' is not equal to 'model_penzl.solution_space.dim'. This is \n",
    "expected since pod_basis.dim equals m (the number of modes, which can vary and is not fixed), whereas model_penzl.solution_space.dim is a fixed integer.\n",
    "'''\n",
    "#space = NumpyVectorSpace(m)\n",
    "#pod_basis = space.make_array(pod_basis_numpy)\n",
    "\n",
    "space = NumpyVectorSpace(model_penzl.order)\n",
    "pod_basis = space.make_array(pod_basis_numpy.T) #This is actually transpose of POD-RB basis\n",
    "\n",
    "# POD-Galerkin RB method\n",
    "pod_reductor = StationaryRBReductor(model_penzl, RB = pod_basis) \n",
    "pod_rom = pod_reductor.reduce()\n",
    "pod_rom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaeba71-9b3d-4120-a2c2-719e91897e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time_reduce = time.time()\n",
    "\n",
    "# Define a parameter space\n",
    "parameter_space = model_penzl.parameters.space(0.01, 10.)\n",
    "\n",
    "# Define a validation set\n",
    "r = 10\n",
    "validation_set = parameter_space.sample_randomly(r)\n",
    "\n",
    "# Solution array containing r many reduced samples\n",
    "reduced_solution = pod_rom.solution_space.empty()\n",
    "for s in validation_set:\n",
    "    reduced_solution.append(pod_rom.solve(s))\n",
    "\n",
    "# End timing\n",
    "end_time_reduce = time.time()\n",
    "\n",
    "# Compute and print the elapsed time\n",
    "elapsed_time_reduce = end_time_reduce - start_time_reduce\n",
    "\n",
    "# Reconstruct high-dimensional vector from reduced vector (necessary for error analysis)\n",
    "reduced_solution_reconstruct = pod_reductor.reconstruct(reduced_solution)\n",
    "\n",
    "print(f'The reconstructed reduced solution matrix (with rows representing the reconstructed reduced solutions for different parameter values) is \\n {reduced_solution_reconstruct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b753586-0ccd-456d-8d6c-528e5b9c98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exact matrix computation\n",
    "\n",
    "# Start timing\n",
    "start_time_exact = time.time()\n",
    "\n",
    "matrixA = to_matrix(penzl.A).toarray()\n",
    "matrixB = to_matrix(penzl.B)\n",
    "\n",
    "# Create a NumPy array containing parameters\n",
    "s_values = np.array([s['s'] for s in validation_set]) \n",
    "\n",
    "# Identity matrix of appropriate size\n",
    "identity = np.eye(penzl.order)\n",
    "\n",
    "exact_solution = np.vstack([(np.linalg.inv(s * identity - matrixA) @ matrixB).flatten() for s in s_values])\n",
    "\n",
    "# End timing\n",
    "end_time_exact = time.time()\n",
    "\n",
    "# Compute and print the elapsed time\n",
    "elapsed_time_exact = end_time_exact - start_time_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee71931-6949-45a9-83b5-95518bea1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error analysis\n",
    "\n",
    "# Start timing\n",
    "start_time_full = time.time()\n",
    "\n",
    "# FOM solution\n",
    "full_solution = model_penzl.solution_space.empty()\n",
    "for s in validation_set:\n",
    "    full_solution.append(model_penzl.solve(s))\n",
    "    \n",
    "# End timing\n",
    "end_time_full = time.time()\n",
    "\n",
    "# Compute and print the elapsed time\n",
    "elapsed_time_full = end_time_full - start_time_full\n",
    "\n",
    "# Convert FOM solution into NumPy array\n",
    "full_solution_numpy = full_solution.to_numpy()\n",
    "\n",
    "# ROM solution\n",
    "reduced_solution_reconstruct_numpy = reduced_solution_reconstruct.to_numpy()\n",
    "\n",
    "# Norms for each training data - > output is an array\n",
    "error_exact = exact_solution - reduced_solution_reconstruct_numpy\n",
    "error_fom = full_solution_numpy - reduced_solution_reconstruct_numpy\n",
    "num_rows = error_exact.shape[0]\n",
    "linfnorm_exact = np.zeros(num_rows)\n",
    "l2norm_exact = np.zeros(num_rows)\n",
    "linfnorm_fom = np.zeros(num_rows)\n",
    "l2norm_fom = np.zeros(num_rows)\n",
    "for row_number in range(num_rows):\n",
    "    linfnorm_exact[row_number] = np.linalg.norm(error_exact[row_number], np.inf)\n",
    "    l2norm_exact[row_number] = np.linalg.norm(error_exact[row_number], 2)  \n",
    "    linfnorm_fom[row_number] = np.linalg.norm(error_fom[row_number], np.inf)\n",
    "    l2norm_fom[row_number] = np.linalg.norm(error_fom[row_number], 2)  \n",
    "\n",
    "# Printing error results\n",
    "print('-'*20 + ' Look below to check if things are going well '+'-'*30)\n",
    "print(f'[The L-infinity norm] The best is {min(linfnorm_exact)} and worst is {max(linfnorm_exact)}.')\n",
    "print(f'[The L-2 norm] The best is {min(l2norm_exact)} and worst is {max(l2norm_exact)}.')\n",
    "print('-'*20 + ' Linf and L2 norms (array) of error vectors '+'-'*30)\n",
    "print(f'The L-infinity norm of (Exact solution - ROM solution) is {linfnorm_exact}.')\n",
    "print(f'The L-infinity norm of (FOM solution - ROM solution) is {linfnorm_fom}.')\n",
    "print(f'The L-2 norm of (Exact solution - ROM solution) is {l2norm_exact}.')\n",
    "print(f'The L-2 norm of (FOM solution - ROM solution) is {l2norm_fom}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30716b2d-ebc2-4e08-8447-a608587a6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational time analysis (execution time analysis)\n",
    "print(f'Total computation time of POD-Galerkin ROM: {elapsed_time_reduce:.10f} seconds.')\n",
    "print(f'Total computation time of FOM: {elapsed_time_full:.10f} seconds.')\n",
    "print(f'Total computation time of Exact: {elapsed_time_exact:.10f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb7e70-10ec-4dfc-a4ec-d2b1e2cfb632",
   "metadata": {},
   "source": [
    "## Summary of POD-Galerkin Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0f94f-36b2-4ca5-9061-415655e4e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrixReductor(A, B, training_set, validation_set, reduced_order: int, reconstruct: bool = False):\n",
    "\n",
    "    # Construct Matrix induced StationaryModel\n",
    "    model_penzl = MatrixModel(A, B)\n",
    "\n",
    "    # Compute FOM solutions for the parameters in the training set\n",
    "    solution_snapshots = model_penzl.solution_space.empty()\n",
    "    for s in training_set:\n",
    "        solution_snapshots.append(model_penzl.solve(s))\n",
    "\n",
    "    # Snapshot matrix S\n",
    "    snapshot_matrix = solution_snapshots.to_numpy().T\n",
    "\n",
    "    # Finding the Singular Value Decomposition (SVD) of the snapshot matrix -> S = UÎ£V^T\n",
    "    U, D, Vt = np.linalg.svd(snapshot_matrix, full_matrices = True)\n",
    "\n",
    "    if reduced_order > min(snapshot_matrix.shape):\n",
    "        raise ValueError(\"'reduced_order' cannot exceed the rank of the snapshot matrix.\")\n",
    "\n",
    "    # The reduced basis (POD basis)\n",
    "    pod_basis_numpy = U[:,:reduced_order]\n",
    "\n",
    "    # Convert NumPy array into VectorArray \n",
    "    space = NumpyVectorSpace(model_penzl.order)\n",
    "    pod_basis = space.make_array(pod_basis_numpy.T) #This is actually transpose of POD-RB basis\n",
    "\n",
    "    # POD-Galerkin RB method\n",
    "    pod_reductor = StationaryRBReductor(model_penzl, RB = pod_basis) \n",
    "    pod_rom = pod_reductor.reduce()\n",
    "\n",
    "    # Solution array containing r many reduced samples\n",
    "    reduced_solution = pod_rom.solution_space.empty()\n",
    "    for s in validation_set:\n",
    "        reduced_solution.append(pod_rom.solve(s))\n",
    "\n",
    "    reduced_solution_reconstruct = pod_reductor.reconstruct(reduced_solution)\n",
    "\n",
    "    if reconstruct is True:\n",
    "        return reduced_solution_reconstruct\n",
    "    else:\n",
    "        return [reduced_solution, pod_reductor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673d8d6-cf7d-41b9-a8a4-3d8b3a5e5738",
   "metadata": {},
   "source": [
    "### Examples (with both real and complex parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e55c3-391f-474d-ad9b-047969659976",
   "metadata": {},
   "source": [
    "#### Real-valued training and validation set\n",
    "In this subsection, we will examine the Matrix Reduction method using a real-valued training set and validation set for real parameters. As an example, we will use `penzl_example` from `pymor.models.examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0e3ee-4712-4fac-9cac-8f5be76f54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.models.examples import penzl_example\n",
    "\n",
    "penzl = penzl_example()\n",
    "model_penzl = MatrixModel(penzl.A, penzl.B)\n",
    "\n",
    "A = penzl.A\n",
    "B = penzl.B\n",
    "parameter_space = model_penzl.parameters.space(0.01, 10.)\n",
    "training_set = parameter_space.sample_randomly(30)\n",
    "validation_set = parameter_space.sample_randomly(10)\n",
    "\n",
    "[reduced_solution, pod_reductor] = MatrixReductor(A, B, training_set, validation_set, reduced_order = 15) # reduced_order must be less than or equal to len(training_set)\n",
    "\n",
    "# Reconstruct high-dimensional vector from reduced vector\n",
    "reduced_solution_reconstruct = pod_reductor.reconstruct(reduced_solution)\n",
    "\n",
    "# One may also get reconstructed vector as 'reduced_solution_reconstruct = MatrixReductor(A, B, dim, training_set, validation_set, reduced_order = 15, reconstruct = True)'\n",
    "\n",
    "# Reconstructed high-dimensional NumPy array (necessary for error analysis)\n",
    "reduced_solution_reconstruct_numpy = reduced_solution_reconstruct.to_numpy()\n",
    "#reduced_solution_reconstruct_numpy\n",
    "reduced_solution_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c33fc-e9aa-40be-beb8-f2442f88c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting matrices to numpy arrays\n",
    "matrixA = to_matrix(penzl.A).toarray()\n",
    "matrixB = to_matrix(penzl.B)\n",
    "\n",
    "# Create a NumPy array containing parameters\n",
    "s_values = np.array([s['s'] for s in validation_set]) \n",
    "\n",
    "# Identity matrix of appropriate size\n",
    "identity = np.eye(penzl.order)\n",
    "\n",
    "# Exact solution\n",
    "exact_solution = np.vstack([(np.linalg.inv(s * identity - matrixA) @ matrixB).flatten() for s in s_values])\n",
    "\n",
    "# Data for an error plot\n",
    "order_list = np.arange(1, 16)\n",
    "linf_list = np.zeros(15)\n",
    "l2_list = np.zeros(15)\n",
    "for i in range(15):\n",
    "    [reduced_solution, pod_reductor] = MatrixReductor(A, B, training_set, validation_set, reduced_order = i)\n",
    "    reduced_solution_reconstruct = pod_reductor.reconstruct(reduced_solution)\n",
    "    reduced_solution_reconstruct_numpy = reduced_solution_reconstruct.to_numpy()\n",
    "    linf_list[i] = np.linalg.norm(exact_solution - reduced_solution_reconstruct_numpy, np.inf)\n",
    "    l2_list[i] = np.linalg.norm(exact_solution - reduced_solution_reconstruct_numpy, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c59c6-bdf9-40f8-b962-21b329d0d019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Error norm plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8), dpi=100)\n",
    "ax.semilogy(order_list[6:], linf_list[6:], label = r\"$L^\\infty$ Norm\", color='red', lw = 0.8, marker = 'x')\n",
    "ax.semilogy(order_list[6:], l2_list[6:], label = r\"$L^2$ Norm\", color='blue', lw = 0.8, marker = 'x')\n",
    "ax.set_title(\"Error norms\")\n",
    "ax.set_xlabel(\"Reduced order\")\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc9c12-716b-47c0-9866-587966bd264f",
   "metadata": {},
   "source": [
    "#### Complex-valued training and validation set (A Vital Step for IRKA)\n",
    "In this subsection, we will examine the Matrix Reduction method using a complex-valued training set and validation set for complex parameters. For the IRKA method, we need to compute matrices at complex-valued interpolation points, and we will check if it works for complex parameters as well. As an example, we will use `penzl_example` from `pymor.models.examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4cdd3-1127-4191-bc96-ecb169a3109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.parameters.base import Mu\n",
    "\n",
    "# Create a complex-valued training set\n",
    "card_training_set = 40\n",
    "complex_parameters = 10*np.random.random_sample((card_training_set,)) + 10*1j*np.random.random_sample((card_training_set,))\n",
    "imaginary_training_set = []\n",
    "for i in range(card_training_set):\n",
    "    imaginary_training_set.append(Mu({'s': np.array(complex_parameters[i])}))\n",
    "\n",
    "# Create a complex-valued validation set\n",
    "card_validation_set = 10\n",
    "complex_parameters_validation = 15*np.random.random_sample((card_training_set,)) + 15*1j*np.random.random_sample((card_training_set,))\n",
    "imaginary_validation_set = []\n",
    "for k in range(10):\n",
    "    imaginary_validation_set.append(Mu({'s': np.array(complex_parameters_validation[k])}))\n",
    "print(f'An complex-valued training set is {imaginary_training_set}.')\n",
    "print(f'An complex-valued validation set is {imaginary_validation_set}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82171b89-7d68-46b5-939a-8706b8fc0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.models.examples import penzl_example\n",
    "\n",
    "penzl = penzl_example()\n",
    "model_penzl = MatrixModel(penzl.A, penzl.B)\n",
    "\n",
    "A = penzl.A\n",
    "B = penzl.B\n",
    "\n",
    "# Reduced solution\n",
    "reduced_order = 15\n",
    "[reduced_solution, pod_reductor] = MatrixReductor(A, B, imaginary_training_set, imaginary_validation_set, reduced_order = reduced_order)\n",
    "\n",
    "# Reconstruct high-dimensional vector from reduced vector\n",
    "reduced_solution_reconstruct = pod_reductor.reconstruct(reduced_solution)\n",
    "\n",
    "# Reconstructed high-dimensional NumPy array (necessary for error analysis)\n",
    "reduced_solution_reconstruct_numpy = reduced_solution_reconstruct.to_numpy()\n",
    "#reduced_solution_reconstruct_numpy\n",
    "reduced_solution_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2912e-f423-4da6-a51c-84e7f3c7fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting matrices to numpy arrays\n",
    "matrixA = to_matrix(penzl.A).toarray()\n",
    "matrixB = to_matrix(penzl.B)\n",
    "\n",
    "# Create a NumPy array containing parameters\n",
    "s_values = np.array([s['s'] for s in imaginary_validation_set]) \n",
    "\n",
    "# Identity matrix of appropriate size\n",
    "identity = np.eye(penzl.order)\n",
    "\n",
    "# Exact solution\n",
    "exact_solution = np.vstack([(np.linalg.inv(s * identity - matrixA) @ matrixB).flatten() for s in s_values])\n",
    "\n",
    "# Data\n",
    "reduced_order_error = 15\n",
    "order_list = np.arange(1, reduced_order_error + 1)\n",
    "linf_list = np.zeros(reduced_order_error)\n",
    "l2_list = np.zeros(reduced_order_error)\n",
    "for i in range(reduced_order_error):\n",
    "    [reduced_solution, pod_reductor] = MatrixReductor(A, B, imaginary_training_set, imaginary_validation_set, reduced_order = i)\n",
    "    reduced_solution_reconstruct = pod_reductor.reconstruct(reduced_solution)\n",
    "    reduced_solution_reconstruct_numpy = reduced_solution_reconstruct.to_numpy()\n",
    "    linf_list[i] = np.linalg.norm(exact_solution - reduced_solution_reconstruct_numpy, np.inf)\n",
    "    l2_list[i] = np.linalg.norm(exact_solution - reduced_solution_reconstruct_numpy, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1516880-6c03-45bc-ba6f-2a0d26017c87",
   "metadata": {},
   "source": [
    "### Hooray!ðŸŽ‰ It also seems to handle complex parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c47b8a-77ca-4ae2-b44a-ce5243cc652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error norm plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8), dpi=100)\n",
    "ax.semilogy(order_list[8:], linf_list[8:], label = r\"$L^\\infty$ Norm\", color='red', lw = 0.8, marker = 'x')\n",
    "ax.semilogy(order_list[8:], l2_list[8:], label = r\"$L^2$ Norm\", color='blue', lw = 0.8, marker = 'x')\n",
    "ax.set_title(\"Error norms\")\n",
    "ax.set_xlabel(\"Reduced order\")\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3bf6d-ea16-4d91-8a0c-2c0c2b82e3ab",
   "metadata": {},
   "source": [
    "### Implementation of Matrix Reduction to IRKA (Single Input Single Output)\n",
    "Consider some initial interpolation data $-\\mu_{i}, \\hat{c}_{i}, \\hat{b}_{i}$ for $0 < r \\leq n$. Here, we construct the projection matrices $V$ and $W$ as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "    V = [(-\\mu_{1}I_{n} - A)^{-1}B\\hat{b}_{1} \\quad (-\\mu_{2}I_{n} - A)^{-1}B\\hat{b}_{2} \\quad \\cdots \\quad (-\\mu_{r}I_{n} - A)^{-1}B\\hat{b}_{r}], \\quad \\text{and} \\quad W = [(-\\mu_{1}I_{n} - A)^{-*}C^{T}\\hat{c}_{1} \\quad (-\\mu_{2}I_{n} - A)^{-*}C^{T}\\hat{c}_{2}\\quad \\cdots \\quad (-\\mu_{r}I_{n} - A)^{-*}C^{T}\\hat{c}_{r}] \\quad \\text{for } i = 1,\\ldots,r,\n",
    "\\end{equation*}\n",
    "where $r$ is the desired order of approximating ROM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653fd380-cbd8-46e3-852d-37ee87a28bda",
   "metadata": {},
   "source": [
    "### Defining Functions for Matrix Reduction in IRKA\n",
    "\n",
    "Here, we will revisit and refine everything we have covered so far. Notice that we have written a `MatrixModel` function and a `MatrixReductor` only for the first linear coercive model. However, we need these functions for both models. Below, we explicitly redefine these functions for both cases. Additionally, we introduce a new function called `ProjectionMatrices`, which provides the projection matrices $V$ and $W$ required for IRKA. This function will streamline the process, making it sufficient to call it directly for use in IRKA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf39f6-f0bd-4480-9044-924d86b532da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrixModel(A, B, C):\n",
    "    '''\n",
    "    This function creates stationary models for the following linear coercive models derived for the given three matrices A, B, and C:\n",
    "    \n",
    "        a_1(v, w; s) = w*(sI_n - A)v and l_1(w) = w*B\n",
    "        a_2(v, w; s) = w*(sI_n - A)*v and l_2(w) = w*C^T.\n",
    "\n",
    "    Inputs:\n",
    "    ------------------------------------------------\n",
    "    A - matrix -> NumPy array or NumpyMatrixOperator\n",
    "    B - vector -> NumPy array or NumpyMatrixOperator\n",
    "    C - vector -> NumPy array or NumpyMatrixOperator\n",
    "    ------------------------------------------------\n",
    "    Outputs:\n",
    "    ------------------------------------------------\n",
    "    model_V - Stationary Model of linear coercive model w*(sI_n - A)v = w*B -> StationaryModel\n",
    "    model_W - Stationary Model of linear coercive model w*(sI_n - A)*v -> StationaryModel\n",
    "    '''\n",
    "\n",
    "    # Define operators (and also a dimension of a model)\n",
    "    if isinstance(A, np.ndarray):\n",
    "        dim = A.shape[0]\n",
    "        A_op = NumpyMatrixOperator(A)\n",
    "    else:\n",
    "        dim = to_matrix(A).shape[0]\n",
    "        A_op = A\n",
    "\n",
    "    if isinstance(B, np.ndarray):\n",
    "        B_op = NumpyMatrixOperator(B) \n",
    "    else:\n",
    "        B_op = B  \n",
    "\n",
    "    if isinstance(C, np.ndarray):\n",
    "        C_op = NumpyMatrixOperator(C.T)  \n",
    "    else:\n",
    "        C_op = C.H  # C is real, so adjoint is transpose\n",
    "\n",
    "    I_op = NumpyMatrixOperator(np.eye(dim))\n",
    "    \n",
    "    # Define parameter functional for 's'\n",
    "    s_param = ProjectionParameterFunctional('s', 1)\n",
    "\n",
    "    # Define bilinear form a(v, w; s) = w*(sI - A)v\n",
    "    a_op_1 = LincombOperator([I_op, A_op], [s_param, -1])\n",
    "\n",
    "    # Define bilinear form a(v, w; s) = w*(sI - A)*v -> Note: (sI - A)* = s*I - A*\n",
    "    a_op_2 = LincombOperator([I_op, A_op.H], [ConjugateParameterFunctional(s_param), -1])\n",
    "    \n",
    "\n",
    "    # Define linear functional l(w) = w^*B\n",
    "    l_op_1 = B_op\n",
    "\n",
    "    # Define linear functional l(w) = w^*C^{T}\n",
    "    l_op_2 = C_op\n",
    "\n",
    "    # Define the StationaryModels\n",
    "    model_V = StationaryModel(operator=a_op_1, rhs=l_op_1)\n",
    "    model_W = StationaryModel(operator=a_op_2, rhs=l_op_2)\n",
    "\n",
    "    return [model_V, model_W] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7535e-173e-4fc5-931b-6a6a93d75eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrixReductor(A, B, C, training_set, validation_set, reduced_order_V: int, reduced_order_W: int, reconstruct: bool = False):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "    ------------------------------------------------\n",
    "    A - matrix -> NumPy array or NumpyMatrixOperator\n",
    "    B - vector -> NumPy array or NumpyMatrixOperator\n",
    "    C - vector -> NumPy array or NumpyMatrixOperator\n",
    "    training_set - an array containing parameters used to construct the snapshot matrix -> type(training_set[i]) = pymor.parameters.base.Mu (list of Mu objects)\n",
    "    validation_set - an array containing parameters used to evaluate the reduced model after its construction -> type(validation_set[i]) = pymor.parameters.base.Mu (list of Mu objects)\n",
    "    reduced_order_V - order of the reduced model for the first coercive model -> int\n",
    "    reduced_order_W - order of the reduced model for the second coercive model -> int\n",
    "    reconstruct - If 'True', the reconstructed reduced solutions for both coercive models will be output; otherwise, the reduced solutions for both coercive models and the reductors for both reduced models will be returned -> bool\n",
    "    ------------------------------------------------\n",
    "    Outputs:\n",
    "    ------------------------------------------------\n",
    "    reduced_solution_reconstruct_V -> NumpyVectorArray\n",
    "    reduced_solution_reconstruct_W -> NumpyVectorArray\n",
    "    reduced_solution_V -> NumpyVectorArray\n",
    "    reduced_solution_W -> NumpyVectorArray\n",
    "    pod_reductor_V -> StationaryRBReductor\n",
    "    pod_reductor_W -> StationaryRBReductor\n",
    "    '''\n",
    "\n",
    "    # Construct Matrix induced StationaryModels\n",
    "    [model_V, model_W] = MatrixModel(A, B, C)\n",
    "\n",
    "    # Compute FOM solutions for the parameters in the training set\n",
    "    solution_snapshots_V = model_V.solution_space.empty()\n",
    "    solution_snapshots_W = model_W.solution_space.empty()\n",
    "    for s in training_set:\n",
    "        solution_snapshots_V.append(model_V.solve(s))\n",
    "        solution_snapshots_W.append(model_W.solve(s))\n",
    "        \n",
    "    # Snapshot matrices\n",
    "    snapshot_matrix_V = solution_snapshots_V.to_numpy().T # Note: One may also use solution_snapshots_V.impl._array.T to get np.ndarray type needed for computation\n",
    "    snapshot_matrix_W = solution_snapshots_W.to_numpy().T\n",
    "\n",
    "    # Finding the Singular Value Decomposition (SVD) of snapshot matrices -> S = UÎ£V^T\n",
    "    U_V, D_V, Vt_V = np.linalg.svd(snapshot_matrix_V, full_matrices = True)\n",
    "    U_W, D_W, Vt_W = np.linalg.svd(snapshot_matrix_W, full_matrices = True)\n",
    "\n",
    "    if reduced_order_V > min(snapshot_matrix_V.shape):\n",
    "        raise ValueError(\"'reduced_order_V' cannot exceed the rank of the snapshot matrix.\")\n",
    "    if reduced_order_W > min(snapshot_matrix_W.shape):\n",
    "        raise ValueError(\"'reduced_order_W' cannot exceed the rank of the snapshot matrix.\")\n",
    "\n",
    "    # The reduced bases (POD bases)\n",
    "    pod_basis_numpy_V = U_V[:,:reduced_order_V]\n",
    "    pod_basis_numpy_W = U_W[:,:reduced_order_W]\n",
    "\n",
    "    # Convert NumPy array into VectorArray \n",
    "    space_V = NumpyVectorSpace(model_V.order)\n",
    "    space_W = NumpyVectorSpace(model_W.order)\n",
    "    pod_basis_V = space_V.make_array(pod_basis_numpy_V.T) #This is actually transpose of POD-RB basis\n",
    "    pod_basis_W = space_W.make_array(pod_basis_numpy_W.T) #This is actually transpose of POD-RB basis\n",
    "    \n",
    "    # POD-Galerkin RB method\n",
    "    pod_reductor_V = StationaryRBReductor(model_V, RB = pod_basis_V) \n",
    "    pod_reductor_W = StationaryRBReductor(model_W, RB = pod_basis_W) \n",
    "    pod_rom_V = pod_reductor_V.reduce()\n",
    "    pod_rom_W = pod_reductor_W.reduce()\n",
    "\n",
    "    # Solution arrays containing len(validation_set) many reduced samples\n",
    "    reduced_solution_V = pod_rom_V.solution_space.empty()\n",
    "    reduced_solution_W = pod_rom_W.solution_space.empty()\n",
    "    for s in validation_set:\n",
    "        reduced_solution_V.append(pod_rom_V.solve(s))\n",
    "        reduced_solution_W.append(pod_rom_W.solve(s))\n",
    "\n",
    "    reduced_solution_reconstruct_V = pod_reductor_V.reconstruct(reduced_solution_V) # a matrix with rows representing the reconstructed reduced solutions for different parameter values to first parametrized coercive model (row i will give us (s_{i}I - A)^{-1}B)\n",
    "    reduced_solution_reconstruct_W = pod_reductor_W.reconstruct(reduced_solution_W) # a matrix with rows representing the reconstructed reduced solutions for different parameter values to second parametrized coercive model (row i will give us (s_{i}I - A)^{-*}C^T)\n",
    "\n",
    "    if reconstruct is True:\n",
    "        return [reduced_solution_reconstruct_V, reduced_solution_reconstruct_W]\n",
    "    else:\n",
    "        return [reduced_solution_V, reduced_solution_W, pod_reductor_V, pod_reductor_W]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c389fa2-2708-4347-9626-5c032f607f25",
   "metadata": {},
   "source": [
    "Let $A\\in\\mathbb{R}^{n\\times n}$ $B\\in\\mathbb{R}^{n\\times 1}$ $C\\in\\mathbb{R}^{1\\times n}$ be given. If one uses `MatrixReductor(A, B, C, training_set, validation_set, reduced_order_V=N, reduced_order_W=M, reconstruct=True)`, then the outputs are `reduced_solution_reconstruct_V = R^{V}_N(s)` and `reduced_solution_reconstruct_W = R^{W}_M(s)`, which are given as:\n",
    "\\begin{equation*}\n",
    "R^{V}_N(s) = \n",
    "\\begin{bmatrix}\n",
    "\\hat{v}^{T}_{N}(s_{1})\\\\\n",
    "\\hat{v}^{T}_{N}(s_{2})\\\\\n",
    "\\vdots\\\\\n",
    "\\hat{v}^{T}_{N}(s_{r})\n",
    "\\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "R^{W}_M(s) = \n",
    "\\begin{bmatrix}\n",
    "\\hat{w}^{T}_{M}(s_{1})\\\\\n",
    "\\hat{w}^{T}_{M}(s_{2})\\\\\n",
    "\\vdots\\\\\n",
    "\\hat{w}^{T}_{M}(s_{r})\n",
    "\\end{bmatrix},\n",
    "\\end{equation*}\n",
    "where $\\hat{v}_{N}(s_{i})\\in\\mathbb{C}^{n\\times 1}$ and $\\hat{w}_{M}(s_{i})\\in\\mathbb{C}^{n\\times 1}$ are the reconstructed solutions of the reduced solutions $v_{N}(s_{i})\\in\\mathbb{C}^{N\\times 1}$ and $w_{M}(s_{i})\\in\\mathbb{C}^{M\\times 1}$ with reduction orders $N$ and $M$, respectively, for the following (parameter-separable) parameterized linear coercive models (FOM) evaluated at a given parameter $s_{i}$.\n",
    "\\begin{align*}\n",
    "    a_{1}(v, u; s) = l_{1}(u) \\quad v, u\\in \\mathbb{C}^{n\\times 1}\\\\\n",
    "    a_{2}(w, u; s) = l_{2}(u) \\quad w, u\\in \\mathbb{C}^{n\\times 1}\n",
    "\\end{align*}\n",
    "where $a_{1}(v, u; s) = u^{*}(sI_{n} - A)v\\in \\mathbb{C}$ and $l_{1}(u) = u^{*}B$, and $a_{2}(w, u; s) = u^{*}(sI_{n} - A)^{*}w\\in \\mathbb{C}$ and $l_{2}(u) = u^{*}C^{T}$, and solutions to these parametrized linear coercive models are\n",
    "\\begin{equation*}\n",
    "    v(s) = (sI_{n} - A)^{-1}B \\quad \\text{and} \\quad w(s) = (sI_{n} - A)^{-*}C^{T}.\n",
    "\\end{equation*}\n",
    "Consider some initial interpolation data $-\\mu_{i}, \\hat{c}_{i}, \\hat{b}_{i}\\in\\mathbb{C}$ for $0 < r \\leq n$. Therefore, the projection matrices are given by\n",
    "\\begin{equation*}\n",
    "    V = \n",
    "    \\begin{bmatrix}\n",
    "    \\hat{v}_{N}(-\\mu_{1})\\hat{b}_{1}&\\hat{v}_{N}(-\\mu_{2})\\hat{b}_{2}&\\cdots&\\hat{v}_{N}(-\\mu_{r})\\hat{b}_{r}\n",
    "    \\end{bmatrix} = R^{V}_N(\\mu)^{T}D_{\\hat{b}} \\quad \\text{and} \\quad\n",
    "    W = \n",
    "    \\begin{bmatrix}\n",
    "    \\hat{w}_{N}(-\\mu_{1})\\hat{c}_{1}&\\hat{w}_{N}(-\\mu_{2})\\hat{c}_{2}&\\cdots&\\hat{w}_{N}(-\\mu_{r})\\hat{c}_{r}\n",
    "    \\end{bmatrix} = R^{W}_M(\\mu)^{T}D_{\\hat{c}},\n",
    "\\end{equation*}\n",
    "where \n",
    "\\begin{equation*}\n",
    "    D_{\\hat{b}} = diag(\\hat{b}_{1}, \\hat{b}_{2}, \\ldots, \\hat{b}_{r}) \\quad \\text{and} \\quad D_{\\hat{c}} = diag(\\hat{c}_{1}, \\hat{c}_{2}, \\ldots, \\hat{c}_{r}).\n",
    "\\end{equation*}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f03ef0-6a2c-4f3c-ac34-913126924c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProjectionMatrices(A, B, C, mu, b, c, training_set, reduced_order_V: int, reduced_order_W: int):\n",
    "\n",
    "    '''\n",
    "    Inputs:\n",
    "    ------------------------------------------------\n",
    "    A - matrix -> NumPy array or NumpyMatrixOperator -> A.shape = (n, n)\n",
    "    B - (column) vector -> NumPy array or NumpyMatrixOperator -> B.shape = (n, m)\n",
    "    C - (row) vector -> NumPy array or NumpyMatrixOperator -> C.shape = (p, n)\n",
    "    mu - list of -mu_i values -> type(mu[i]) = pymor.parameters.base.Mu (list of Mu objects)\n",
    "    b - NumPy array -> b.shape = (r,) where r = len(mu)\n",
    "    c - NumPy array -> c.shape = (r,) where r = len(mu)\n",
    "    ------------------------------------------------\n",
    "    Outputs:\n",
    "    ------------------------------------------------\n",
    "    V - projection matrix V -> NumpyVectorArray -> V.shape = (n, r)\n",
    "    W - projection matrix W -> NumpyVectorArray -> W.shape = (n, r)\n",
    "    '''\n",
    "\n",
    "    [R_V, R_W] = MatrixReductor(A = A, B = B, C = C, training_set = training_set, validation_set = mu, reduced_order_V = reduced_order_V, reduced_order_W = reduced_order_W, reconstruct = True)\n",
    "\n",
    "    R_V, R_W = R_V.to_numpy(), R_W.to_numpy() # Note: One may also use R_V.impl._array to get np.ndarray type needed for computation\n",
    "    D_b, D_c = np.diag(b), np.diag(c)\n",
    "\n",
    "    V_numpy = np.matmul(R_V.T, D_b)\n",
    "    W_numpy = np.matmul(R_W.T, D_c)\n",
    "\n",
    "    space = NumpyVectorSpace(V_numpy.shape[1])\n",
    "    V = space.make_array(V_numpy)\n",
    "    W = space.make_array(W_numpy)\n",
    "\n",
    "    return [V, W]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14393ae3-7adf-4142-a166-1cedd121d89d",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Here, we are going to look at the case when $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times m}$, and $C \\in \\mathbb{R}^{p \\times n}$, where $m, p > 1$. This is an important example as it will help us understand the MIMO (Multiple Input, Multiple Output) case for LTI systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9a5e2-801a-4c77-b107-ef8e4ce9204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining matrices\n",
    "n = 150\n",
    "m = 3 # number of inputs\n",
    "p = 4 # number of outputs\n",
    "\n",
    "# A: n x n matrix with normal distribution\n",
    "np.random.seed(10)\n",
    "A = np.random.normal(size = (n,n)) \n",
    "\n",
    "# B: n x m matrix with normal distribution\n",
    "np.random.seed(25)\n",
    "B = np.random.normal(size = (n,m))\n",
    "\n",
    "# C: p x n matrix with normal distribution\n",
    "np.random.seed(45)\n",
    "C = np.random.normal(size = (p,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f42797-3214-4caa-b95a-e9d978dfcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.parameters.base import Mu\n",
    "\n",
    "# Create a complex-valued training set\n",
    "card_training_set = 30\n",
    "complex_parameters = 10*np.random.normal(size = (card_training_set,)) + 10*1j*np.random.normal(size = (card_training_set,))\n",
    "imaginary_training_set = []\n",
    "for i in range(card_training_set):\n",
    "    imaginary_training_set.append(Mu({'s': np.array(complex_parameters[i])}))\n",
    "\n",
    "# Create a complex-valued validation set\n",
    "card_validation_set = 15\n",
    "complex_parameters_validation = np.random.normal(size = (card_training_set,)) + 15*1j*np.random.normal(size = (card_training_set,))\n",
    "imaginary_validation_set = []\n",
    "for k in range(10):\n",
    "    imaginary_validation_set.append(Mu({'s': np.array(complex_parameters_validation[k])}))\n",
    "print(f'An complex-valued training set is {imaginary_training_set}.')\n",
    "print(f'An complex-valued validation set is {imaginary_validation_set}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48852e-f3f3-418b-8473-d44c1828fcd6",
   "metadata": {},
   "source": [
    "## Caution!\n",
    "Note that for the MIMO case, an error arises when we construct the StationaryModel because `rhs.source.is_scalar` is False. This occurs because $v$ and $w$ are matrices, not vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67327f4-4071-487d-ac94-89b25fd27b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatrixModel(A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4bad7a-47d8-4283-ad98-cc9da646fc49",
   "metadata": {},
   "source": [
    "## Remedy\n",
    "The right question would be whether it is possible to split the model into submodels with `rhs.source.is_scalar = True`, or in other words, into models with \\( m = p = 1 \\). In this context, one would need to check if it is possible to partition the MIMO model into SISO models. It turns out that this is indeed possible, and several methodologies exist for doing so. One commonly used method to split a MIMO model into SISO models is Singular Value Decomposition (SVD). In this discussion, we will not focus on the MIMO case, as SVD can be employed to obtain several SISO models, and we can apply the method we proposed for each individual SISO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11fb31-4042-478b-ae52-ae03db40f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.parameters.base import Mu\n",
    "\n",
    "# Redefining number inputs and outputs (as we will deal with SISO model)\n",
    "n = 200\n",
    "m = 1 # number of inputs\n",
    "p = 1 # number of outputs\n",
    "\n",
    "# A: n x n matrix with normal distribution\n",
    "np.random.seed(10)\n",
    "A = np.random.normal(size = (n,n)) \n",
    "\n",
    "# B: n x m matrix with normal distribution\n",
    "np.random.seed(25)\n",
    "B = np.random.normal(size = (n,m))\n",
    "\n",
    "# C: p x n matrix with normal distribution\n",
    "np.random.seed(45)\n",
    "C = np.random.normal(size = (p,n))\n",
    "\n",
    "# Interpolation data\n",
    "num_points = 10\n",
    "np.random.seed(39)\n",
    "mu_single = np.random.normal(size = (num_points,)) + 5*1j*np.random.normal(size = (num_points,))\n",
    "mu = []\n",
    "for k in range(num_points):\n",
    "    mu.append(Mu({'s': np.array(mu_single[k])}))\n",
    "\n",
    "np.random.seed(43)\n",
    "b = 0.1*np.random.normal(size = (num_points,)) + 1j*np.random.normal(size = (num_points,))\n",
    "\n",
    "np.random.seed(47)\n",
    "c = 0.1*np.random.normal(size = (num_points,)) + 1j*np.random.normal(size = (num_points,))\n",
    "\n",
    "# A complex-valued training set\n",
    "card_training_set = 30\n",
    "np.random.seed(52)\n",
    "complex_parameters = 0.4*np.random.normal(size = (card_training_set,)) + 2*1j*np.random.normal(size = (card_training_set,))\n",
    "imaginary_training_set = []\n",
    "for i in range(card_training_set):\n",
    "    imaginary_training_set.append(Mu({'s': np.array(complex_parameters[i])}))\n",
    "\n",
    "[V, W] = ProjectionMatrices(A = A, B = B, C = C, mu = mu, b = b, c = c, training_set = imaginary_training_set, reduced_order_V = 10, reduced_order_W = 10)\n",
    "print(f'The projection matrices are \\n V = {V} \\n and \\n W = {W}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1177f-1635-43e5-9b41-5ae5221740c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb915d9-113c-4b34-b7c7-fcf262f776e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymor.models.examples import penzl_example\n",
    "\n",
    "penzl = penzl_example()\n",
    "\n",
    "[V_penzl, W_penzl] = ProjectionMatrices(A = penzl.A, B = penzl.B, C = penzl.C, mu = mu, b = b, c = c, training_set = imaginary_training_set, reduced_order_V = 10, reduced_order_W = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746587a-5360-41e1-b8e0-b04ea01bf143",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The projection matrices for penzl are \\n V = {V_penzl} \\n and \\n W = {W_penzl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1bfc81-708b-4066-843a-5bbe1cb4200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[v, w] = gram_schmidt_biorth(V, W, check_tol = 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abb591-79cd-45e6-aec1-db6a8ed51390",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.to_numpy()[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842ad3f-deb5-478b-8f10-2e9b3c46caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.matmul(W.impl._array.T, V.impl._array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cce4ac-e146-485f-931d-f0d761f799e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(f) # if W^{T}V = I, then it can be replaced by invertibly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b808f4-21a2-4363-8464-965bec5239f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import cond\n",
    "cond(f) # ill-conditioned matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
